<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> NEVIS | James Rainey </title> <meta name="author" content="James Rainey"> <meta name="description" content="&lt;b&gt;NE&lt;/b&gt;uromorphic &lt;b&gt;VI&lt;/b&gt;sion &lt;b&gt;S&lt;/b&gt;ystem"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jrainey12.github.io/projects/NEVIS.html"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">James</span> Rainey </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <h1 align="center"> <b>NE</b>uromorphic <b>VI</b>sion <b>S</b>ystem (<b>NEVIS</b>)</h1> <hr> </h1> <p class="post-description"></p> <h2 align="center">Building Brain-Inspired Vision Systems That See, and Think, Like We Do</h2> </header> <article> <style>h2{color:#429435;font-size:180%}</style> <hr> <p>The human brain remains one of the best vision systems on the planet. It recognises patterns, adapts to change, and knows instantly when something looks ‚Äúoff.‚Äù Modern AI vision systems, on the other hand, often stumble over these things. They‚Äôre powerful, but also rigid, power-hungry, and not great at admitting when they‚Äôre wrong.</p> <p>That contrast inspired our recent research, presented in two papers at <strong>SPIE 2024</strong> Conferences:</p> <ol> <li> <p><em>‚ÄúDigit Classification using Biologically Plausible Neuromorphic Vision‚Äù</em> <a class="citation" href="#maier2024digit">(Maier et al., 2024)</a> where we taught a spiking neural network to see digits like the brain does, and</p> </li> <li> <p><em>‚ÄúAn FPGA-based Neuromorphic Vision System Accelerator‚Äù</em> <a class="citation" href="#ali2024fpga">(Ali et al., 2024)</a> where we brought that idea to life on real, low-power hardware.</p> </li> </ol> <p>Together, they show how we can make machines that don‚Äôt just <em>see</em> like us, but also <em>think</em> more like us.</p> <p><br></p> <h2> üëÅÔ∏è Seeing the World in Edges, Not Pixels</h2> <hr> <p>In our first paper, we focused on how to give an AI vision model a more <strong>biological way of seeing</strong>. The human visual cortex doesn‚Äôt start with whole objects, it starts with edges and orientations. Neurons in the brain fire for horizontal or vertical lines, and higher layers gradually assemble those into shapes and objects.</p> <p>We applied the same logic to a <strong>spiking neural network (SNN)</strong> trained on the <strong>MNIST</strong> handwritten digits dataset. Before the network even began learning, each image passed through simple <strong>Prewitt filters</strong> that extract edges. Instead of raw brightness values, the SNN saw lines and contours, the same way your brain does.</p> <p>This approach made the network‚Äôs learning more stable and interpretable. And when we added a twist, resetting about 10% of its neurons back to an untrained state after learning, something interesting happened. Those ‚Äúuntrained‚Äù neurons acted like <strong>error detectors</strong>, firing only when the input didn‚Äôt belong to any known category. Suddenly, our system wasn‚Äôt just classifying digits, it could tell when something <em>wasn‚Äôt</em> a digit at all.</p> <p>On tests with random images from the CIFAR-100 dataset, the edge-based, partially untrained model rejected over <strong>96%</strong> of non-digit inputs correctly, compared to near-zero for the pixel-based version. It became a system that knew what it knew, and what it didn‚Äôt.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/NEVIS/28x28-epoch1-1000-480.webp 480w,/assets/img/NEVIS/28x28-epoch1-1000-800.webp 800w,/assets/img/NEVIS/28x28-epoch1-1000-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/NEVIS/28x28-epoch1-1000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="1000 Epochs" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/NEVIS/28x28-epoch1-3000-480.webp 480w,/assets/img/NEVIS/28x28-epoch1-3000-800.webp 800w,/assets/img/NEVIS/28x28-epoch1-3000-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/NEVIS/28x28-epoch1-3000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="3000 Epochs" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/NEVIS/28x28-epoch1-6000-reset10-480.webp 480w,/assets/img/NEVIS/28x28-epoch1-6000-reset10-800.webp 800w,/assets/img/NEVIS/28x28-epoch1-6000-reset10-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/NEVIS/28x28-epoch1-6000-reset10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="6000 epochs, 10 neurons reset" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> 100 neurons trained for 1000 images (left) and 3000 images (centre), and trained on 6000 images with 10 neurons weights reset to random values (right). </div> <p><br></p> <h2>üß† Bringing the Brain to Hardware</h2> <hr> <p>Of course, building a clever algorithm is one thing. Running it efficiently in the real world is another.</p> <p>In the second paper, we took our neuromorphic vision model and built it on a <strong>Xilinx Kria KV260 FPGA</strong>, creating what we call <strong>NEVIS (NEuromorphic VIsion System)</strong>. The goal: make the system not only <em>biologically plausible</em> but also <em>practically efficient.</em></p> <p>FPGAs (Field-Programmable Gate Arrays) are reconfigurable chips that can perform thousands of operations in parallel, perfect for simulating spiking neurons that all fire at once. On the FPGA, NEVIS processed images about <strong>40√ó faster</strong> and used <strong>less power</strong> than a standard <strong>Raspberry Pi 4B</strong>, even though the Pi‚Äôs CPU runs at a much higher clock speed.</p> <p>Here‚Äôs how it stacked up:</p> <div align="center"> <table> <thead> <tr> <th>Platform</th> <th>Inference Time</th> <th>Power Use</th> <th>Speed Gain</th> </tr> </thead> <tbody> <tr> <td>Raspberry Pi 4B</td> <td>1,550 ms</td> <td>6.4 W</td> <td>‚Äî</td> </tr> <tr> <td>Xilinx Kria KV260 FPGA</td> <td>38 ms</td> <td>3.8 W</td> <td><strong>‚âà 40√ó faster</strong></td> </tr> </tbody> </table> </div> <p><br> That speed-up comes from the FPGA‚Äôs natural parallelism, every neuron and synapse can compute simultaneously, just like a biological brain.</p> <div style="padding-left:20%; padding-right:20%;"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/NEVIS/kria-480.webp 480w,/assets/img/NEVIS/kria-800.webp 800w,/assets/img/NEVIS/kria-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/NEVIS/kria.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kria FPGA" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Xilinx Kria KV260 FPGA. </div> <p><br></p> <h2>üîçWhy This Matters</h2> <hr> <p>Taken together, these two studies show how <strong>neuromorphic computing</strong> can lead to systems that are not only faster and more efficient but also more <em>aware</em> of what they‚Äôre seeing.</p> <p>By combining biologically inspired learning with energy-efficient hardware, we‚Äôre getting closer to vision systems that could power small, autonomous devices: drones that navigate intuitively, cameras that understand context, or wearables that interpret their surroundings in real time.</p> <p>In short, this is a step toward machines that see with structure, think with spikes, and know their own limits, a more human kind of intelligence, built from silicon and science.</p> <p><br></p> <h2>üîë Key Takeaways</h2> <hr> <ul> <li> <p><strong>Brains do it better, and smarter.</strong> By mimicking how the visual cortex processes edges and patterns, neuromorphic vision systems can learn more efficiently and handle unfamiliar data with greater robustness.</p> </li> <li> <p><strong>Untrained neurons can be useful.</strong> Resetting a small portion of neurons after training helped the model recognise when it was seeing something new, adding a sense of ‚Äúuncertainty awareness‚Äù that most AI systems lack.</p> </li> <li> <p><strong>Edge-based preprocessing improves stability.</strong> Feeding the network structured, edge-detected inputs (instead of raw pixels) led to smoother learning and better generalisation.</p> </li> <li> <p><strong>Hardware matters.</strong> Running the same model on an FPGA delivered ~40√ó faster performance and better energy efficiency than a Raspberry Pi 4B, proving that brain-inspired AI can be practical on small, low-power devices.</p> </li> <li> <p><strong>Towards smarter, humbler AI.</strong> Combining biologically inspired algorithms with efficient hardware is a promising path toward AI that can not only see but also understand its own limits, much like the human brain.</p> </li> </ul> <p><br></p> <h2> üîÆ Looking Ahead </h2> <hr> <p>The next challenge is to go beyond simple edge detection and digits. The human visual system processes hundreds of orientations, textures, and motion cues across many layers. Expanding NEVIS to capture that richness, and doing so efficiently on hardware, is the natural way to progress.</p> <p>The more we learn from the brain, the closer we get to building machines that not only <em>see</em> the world but truly <em>understand</em> it. <br> <br></p> </article> <h2>References</h2> <div class="publications"> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="maier2024digit" class="col-sm-8"> <div class="title">Digit classification using biologically plausible neuromorphic vision</div> <div class="author"> Patrick Maier, <em>James Rainey</em>, Elena Gheorghiu, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Kofi Appiah, Deepayan Bhowmik' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In Applications of Digital Image Processing XLVII</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/13137/131370I/Digit-classification-using-biologically-plausible-neuromorphic-vision/10.1117/12.3031280.short" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="ali2024fpga" class="col-sm-8"> <div class="title">An FPGA-based neuromorphic vision system accelerator</div> <div class="author"> Teymoor Ali, <em>James Rainey</em>, Sook Yen Lau, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Elena Gheorghiu, Patrick Maier, Kofi Appiah, Deepayan Bhowmik' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>In Artificial Intelligence for Security and Defence Applications II</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/13206/132060F/An-FPGA-based-neuromorphic-vision-system-accelerator/10.1117/12.3034095.short" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> </div> </div> </li> </ol> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2025 James Rainey. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>